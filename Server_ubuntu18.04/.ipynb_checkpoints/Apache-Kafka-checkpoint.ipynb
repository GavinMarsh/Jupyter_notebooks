{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardware requirements:\n",
    "\n",
    "Minimum 2GB of Ram (Optimum is 6GB).\n",
    "\n",
    "## Update system packages\n",
    "    sudo apt update && sudo apt upgrade\n",
    "\n",
    "There are services on your system that need to be restarted select `yes`, and select to `keep all local versions currently installed` - this is incase AWS have changed any config files to ssh onto the machine etc.\n",
    "\n",
    "## Install Java JDK\n",
    "\n",
    "    sudo apt install default-jdk\n",
    "\n",
    "\n",
    "## Creating a User for Kafka\n",
    "Since Kafka can handle requests over a network, you should create a dedicated user for it. This minimizes damage to your Ubuntu machine should the Kafka server be compromised. We will create a dedicated kafka user in this step, but you should create a different non-root user to perform other tasks on this server once you have finished setting up Kafka.\n",
    "\n",
    "Logged in as your non-root sudo user, create a user called kafka with the `useradd` command:\n",
    "\n",
    "    sudo useradd kafka -m\n",
    "\n",
    "The `-m` flag ensures that a home directory will be created for the user. This home directory, `/home/kafka` will act as our workspace directory for executing commands in the sections below.\n",
    "\n",
    "Set the password using `passwd`:\n",
    "\n",
    "    sudo passwd kafka\n",
    "\n",
    "Add the kafka user to the `sudo` group with the `adduser` command, so that it has the privileges required to install Kafka’s dependencies:\n",
    "\n",
    "    sudo adduser kafka sudo\n",
    "\n",
    "Your kafka user is now ready. Log into this account using `su`:\n",
    "\n",
    "    su -l kafka\n",
    "\n",
    "\n",
    "## Download and Extract Kafka Binaries\n",
    "\n",
    "Let’s download and extract the Kafka binaries into dedicated folders in our kafka user’s home directory.\n",
    "\n",
    "    mkdir ~/Downloads\n",
    "\n",
    "Use `wget` to download the Kafka binaries:\n",
    "\n",
    "    wget http://www-us.apache.org/dist/kafka/2.5.0/kafka_2.13-2.5.0.tgz -O ~/Downloads/kafka.tgz\n",
    "\n",
    "Create a directory called `kafka` and change to this directory. This will be the base directory of the Kafka installation:\n",
    "\n",
    "    mkdir ~/kafka && cd ~/kafka\n",
    "\n",
    "Extract the archive you downloaded using the `tar` command:\n",
    "\n",
    "    tar -xvzf ~/Downloads/kafka.tgz --strip 1\n",
    "\n",
    "We specify the `--strip 1` flag to ensure that the archive’s contents are extracted in `~/kafka/` itself and not in another directory (such as `~/kafka/kafka_2.11-2.1.1/`) inside of it.\n",
    "\n",
    "Now that we’ve downloaded and extracted the binaries successfully, we can move on configuring to Kafka to allow for topic deletion.\n",
    "\n",
    "\n",
    "## Configure the Kafka Server\n",
    "\n",
    "Kafka’s default behavior will not allow us to delete a topic, the category, group, or feed name to which messages can be published. To modify this, let’s edit the configuration file.\n",
    "\n",
    "Kafka’s configuration options are specified in `server.properties`. Open this file with `nano` or your favorite editor:\n",
    "\n",
    "    nano ~/kafka/config/server.properties\n",
    "\n",
    "Let’s add a setting that will allow us to delete Kafka topics. Add the following to the bottom of the file:\n",
    "\n",
    "    delete.topic.enable = true\n",
    "\n",
    "Save the file, and exit `nano`. Now that we’ve configured Kafka, we can move on to creating systemd unit files for running and enabling it on startup.\n",
    "\n",
    "\n",
    "## Create Systemd Unit Files and Starting the Kafka Server\n",
    "\n",
    "In this section, we will create systemd unit files for the Kafka service. This will help us perform common service actions such as starting, stopping, and restarting Kafka in a manner consistent with other Linux services.\n",
    "\n",
    "Zookeeper is a service that Kafka uses to manage its cluster state and configurations. It is commonly used in many distributed systems as an integral component. If you would like to know more about it, visit the official Zookeeper docs.\n",
    "\n",
    "Create the unit file for `zookeeper`:\n",
    "\n",
    "    sudo nano /etc/systemd/system/zookeeper.service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Unit]\n",
    "Requires=network.target remote-fs.target\n",
    "After=network.target remote-fs.target\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "User=kafka\n",
    "ExecStart=/home/kafka/kafka/bin/zookeeper-server-start.sh /home/kafka/kafka/config/zookeeper.properties\n",
    "ExecStop=/home/kafka/kafka/bin/zookeeper-server-stop.sh\n",
    "Restart=on-abnormal\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Unit] section specifies that Zookeeper requires networking and the filesystem to be ready before it can start.\n",
    "\n",
    "The [Service] section specifies that systemd should use the zookeeper-server-start.sh and zookeeper-server-stop.sh shell files for starting and stopping the service. It also specifies that Zookeeper should be restarted automatically if it exits abnormally.\n",
    "\n",
    "Next, create the systemd service file for `kafka`:\n",
    "\n",
    "    sudo nano /etc/systemd/system/kafka.service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Unit]\n",
    "Requires=zookeeper.service\n",
    "After=zookeeper.service\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "User=kafka\n",
    "ExecStart=/bin/sh -c '/home/kafka/kafka/bin/kafka-server-start.sh /home/kafka/kafka/config/server.properties > /home/kafka/kafka/kafka.log 2>&1'\n",
    "ExecStop=/home/kafka/kafka/bin/kafka-server-stop.sh\n",
    "Restart=on-abnormal\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `[Unit]` section specifies that this unit file depends on `zookeeper.service`. This will ensure that `zookeeper` gets started automatically when the `kafka` service starts.\n",
    "\n",
    "The `[Service]` section specifies that systemd should use the `kafka-server-start`.sh and `kafka-server-stop.sh` shell files for starting and stopping the service. It also specifies that Kafka should be restarted automatically if it exits abnormally.\n",
    "\n",
    "The new system units have been added, so let’s enable Apache Kafka to automatically run on boot, and then run the service.\n",
    "\n",
    "    sudo systemctl enable kafka\n",
    "\n",
    "    sudo systemctl start kafka\n",
    "\n",
    "\n",
    "To ensure that the server has started successfully, check the journal logs for the `kafka` unit:\n",
    "\n",
    "    sudo journalctl -u kafka\n",
    "\n",
    "You should see output similar to the following:\n",
    "\n",
    "    Jul 17 18:38:59 kafka-ubuntu systemd[1]: Started kafka.service.\n",
    "\n",
    "You now have a Kafka server listening on port `9092`.\n",
    "\n",
    "## If server returns an error you can check the journal to read the errors:\n",
    "\n",
    "    journalctl -xe\n",
    "\n",
    "Then Reload the daemon.\n",
    "\n",
    "    'systemctl daemon-reload'\n",
    "\n",
    "## Test the Installation\n",
    "\n",
    "Let’s publish and consume a “Hello World” message to make sure the Kafka server is behaving correctly. Publishing messages in Kafka requires:\n",
    "\n",
    "A producer, which enables the publication of records and data to topics.\n",
    "A consumer, which reads messages and data from topics.\n",
    "First, create a topic named `TutorialTopic` by typing:\n",
    "\n",
    "    ~/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic TutorialTopic\n",
    "\n",
    "You can create a producer from the command line using the kafka-console-producer.sh script. It expects the Kafka server’s hostname, port, and a topic name as arguments.\n",
    "\n",
    "Publish the string \"Hello, World\" to the `TutorialTopic` topic by typing:\n",
    "\n",
    "    echo \"Hello, World\" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic TutorialTopic > /dev/null\n",
    "\n",
    "Next, you can create a Kafka consumer using the `kafka-console-consumer.sh` script. It expects the ZooKeeper server’s hostname and port, along with a topic name as arguments.\n",
    "\n",
    "The following command consumes messages from `TutorialTopic`. Note the use of the `--from-beginning` flag, which allows the consumption of messages that were published before the consumer was started:\n",
    "\n",
    "    ~/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic TutorialTopic --from-beginning\n",
    "\n",
    "If there are no configuration issues, you should see `Hello, World` in your terminal:\n",
    "\n",
    "The script will continue to run, waiting for more messages to be published to the topic. Feel free to open a new terminal and start a producer to publish a few more messages. You should be able to see them all in the consumer’s output.\n",
    "\n",
    "When you are done testing, press `CTRL+C` to stop the consumer script. Now that we have tested the installation, let’s move on to installing KafkaT.\n",
    "\n",
    "    \n",
    "## Install KafkaT (Optional)\n",
    "\n",
    "KafkaT is a tool from Airbnb that makes it easier for you to view details about your Kafka cluster and perform certain administrative tasks from the command line. Because it is a Ruby gem, you will need Ruby to use it. You will also need the `build-essential` package to be able to build the other gems it depends on. Install them using `apt`:\n",
    "\n",
    "    sudo apt install ruby ruby-dev build-essential\n",
    "\n",
    "You can now install KafkaT using the gem command:\n",
    "\n",
    "    sudo gem install kafkat\n",
    "\n",
    "KafkaT uses `.kafkatcfg` as the configuration file to determine the installation and log directories of your Kafka server. It should also have an entry pointing KafkaT to your ZooKeeper instance.\n",
    "\n",
    "Create a new file called `.kafkatcfg`:\n",
    "\n",
    "    nano ~/.kafkatcfg\n",
    "\n",
    "Add the following lines to specify the required information about your Kafka server and Zookeeper instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"kafka_path\": \"~/kafka\",\n",
    "  \"log_path\": \"/tmp/kafka-logs\",\n",
    "  \"zk_path\": \"localhost:2181\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to use KafkaT. For a start, here’s how you would use it to view details about all Kafka partitions:\n",
    "\n",
    "    kafkat partitions\n",
    "\n",
    "You will see `TutorialTopic`, as well as `__consumer_offsets`, an internal topic used by Kafka for storing client-related information. You can safely ignore lines starting with `__consumer_offsets`.\n",
    "\n",
    "To learn more about KafkaT, refer to its GitHub repository (https://github.com/airbnb/kafkat)ex\n",
    "\n",
    "\n",
    "## Setting Up a Multi-Node Cluster (Optional)\n",
    "\n",
    "If you want to create a multi-broker cluster using more Ubuntu 18.04 machines, you should repeat Step 1, Step 4, and Step 5 on each of the new machines. Additionally, you should make the following changes in the `server.properties`file for each:\n",
    "\n",
    "The value of the `broker.id` property should be changed such that it is unique throughout the cluster. This property uniquely identifies each server in the cluster and can have any string as its value. For example, `\"server1\"`, `\"server2\"`, etc.\n",
    "\n",
    "The value of the `zookeeper.connect` property should be changed such that all nodes point to the same ZooKeeper instance. This property specifies the Zookeeper instance’s address and follows the `<HOSTNAME/IP_ADDRESS>:<PORT>` format. For example, `\"203.0.113.0:2181\", \"203.0.113.1:2181\"` etc.\n",
    "\n",
    "If you want to have multiple ZooKeeper instances for your cluster, the value of the `zookeeper.connect` property on each node should be an identical, comma-separated string listing the IP addresses and port numbers of all the ZooKeeper instances.\n",
    "\n",
    "\n",
    "## Restricting the Kafka User\n",
    "Now that all of the installations are done, you can remove the kafka user’s admin privileges. Before you do so, log out and log back in as any other non-root sudo user. If you are still running the same shell session you started this tutorial with, simply type `exit`.\n",
    "\n",
    "Remove the kafka user from the sudo group:\n",
    "\n",
    "    sudo deluser kafka sudo\n",
    "\n",
    "To further improve your Kafka server’s security, lock the kafka user’s password using the `passwd` command. This makes sure that nobody can directly log into the server using this account:\n",
    "\n",
    "    sudo passwd -l kafka\n",
    "\n",
    "At this point, only root or a sudo user can log in as `kafka` by typing in the following command:\n",
    "\n",
    "    sudo su - kafka\n",
    "\n",
    "In the future, if you want to unlock it, use `passwd` with the `-u` option:\n",
    "\n",
    "    sudo passwd -u kafka\n",
    "\n",
    "You have now successfully restricted the kafka user’s admin privileges.\n",
    "\n",
    "    \n",
    "# Conclusion\n",
    "\n",
    "You now have Apache Kafka running securely on your Ubuntu server. You can make use of it in your projects by creating Kafka producers and consumers using Kafka clients, which are available for most programming languages. To learn more about Kafka, you can also consult its documentation.\n",
    "\n",
    "\n",
    "## Install Maven (Optional)\n",
    "\n",
    "    sudo apt install maven\n",
    "    \n",
    "## Install Kafka-python \n",
    "\n",
    "    conda install -c conda-forge kafka-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
